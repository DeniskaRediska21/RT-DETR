{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natsort in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (8.4.0)\n",
      "Requirement already satisfied: tqdm in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: transformers in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (4.46.0)\n",
      "Requirement already satisfied: torch in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (0.20.0)\n",
      "Requirement already satisfied: filelock in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/env_tensorflow/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install natsort tqdm transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 16:58:10.786161: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-29 16:58:10.786179: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torchvision.io import decode_image\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(os.sep, 'ml', 'LIZA_dataset_split', '**')\n",
    "POSITIVE_PATH = os.path.join(os.sep, 'ml', 'LIZA_dataset_forest')\n",
    "NEGATIVE_PATH = os.path.join(os.sep, 'ml', 'LIZA_dataset_urban (not needed)')  # for visual control\n",
    "\n",
    "for path in (POSITIVE_PATH, NEGATIVE_PATH):\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "POSITIVE_LABELS = ['forest', 'steppe', 'fields', 'snowy forest', 'snowy steppe', 'woods', 'taiga', 'few cars', 'road', \n",
    "                   'many green trees', 'big trees', 'grass', 'bareland', 'grassland', 'farmland', 'river', 'lake', \n",
    "                   'hills', 'highway', 'runway', 'tree', 'meadow', 'swamp', 'bushes', 'dead bushes', 'dead tree', \n",
    "                   'person', 'pedestrian', 'human']\n",
    "NEGATIVE_LABELS = ['city', 'town', 'several buildings', 'many buildings', 'aiport', 'many houses', 'terminal', \n",
    "                   'building', 'house']\n",
    "LABELS = POSITIVE_LABELS + NEGATIVE_LABELS\n",
    "TEXT_QUERIES = [f'an aerial photograph of {label}' for label in LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"flax-community/clip-rsicd\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133921, 133921)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [file for file in glob.glob(os.path.join(DATASET_PATH, '*'), recursive=True) if re.match(r'(.*\\.jpg)', file)]\n",
    "annotations = [file for file in glob.glob(os.path.join(DATASET_PATH, '*'), recursive=True) if re.match(r'(.*\\.txt)', file)]\n",
    "\n",
    "images = natsorted(images)\n",
    "annotations = natsorted(annotations)\n",
    "\n",
    "len(images), len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21267it [05:58, 59.25it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ml/LIZA_dataset_split/1_005601_123.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m POSITIVE_LABELS:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m (image, annotation):\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOSITIVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m (image, annotation):\n",
      "File \u001b[0;32m~/anaconda3/envs/env_tensorflow/lib/python3.10/shutil.py:417\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    416\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 417\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/anaconda3/envs/env_tensorflow/lib/python3.10/shutil.py:254\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    257\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ml/LIZA_dataset_split/1_005601_123.jpg'"
     ]
    }
   ],
   "source": [
    "stems  = []\n",
    "for image, annotation in tqdm(zip(images, annotations)):\n",
    "    if Path(image).stem != Path(annotation).stem:\n",
    "        raise ValueError('Images and annotations indices do not match')\n",
    "    \n",
    "    T = decode_image(image)\n",
    "    \n",
    "    X = processor(text=TEXT_QUERIES, images=T, return_tensors='pt', padding=True)\n",
    "    Y = model(**X.to(device))\n",
    "    \n",
    "    label = LABELS[torch.argmax(Y.logits_per_image.softmax(dim=1).cpu().detach()[0])]\n",
    "    if label in POSITIVE_LABELS:\n",
    "        for file in (image, annotation):\n",
    "            shutil.copy(file, POSITIVE_PATH)\n",
    "    else:\n",
    "        for file in (image, annotation):\n",
    "            shutil.copy(file, NEGATIVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensorflow",
   "language": "python",
   "name": "env_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
